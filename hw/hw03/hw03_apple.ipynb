{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw03_apple.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Text Analysis of Bloomberg Articles\n",
    "\n",
    "## Data Cleaning and EDA\n",
    "\n",
    "### Due Date: Thursday, September 26, 11:59 PM\n",
    "\n",
    "You must submit this assignment to Gradescope by the on-time deadline, **Thursday, September 26**, at 11:59 PM. Please read the syllabus for the Slip Day policy. No late submissions beyond what is outlined in the Slip Day policy will be accepted. **We strongly encourage you to submit your work to Gradescope several hours before the stated deadline.** This way, you will have ample time to reach out to staff for support if you encounter difficulties with submission. While course staff is happy to help guide you with submitting your assignment ahead of the deadline, we will not respond to last-minute requests for assistance.\n",
    "\n",
    "Please read the instructions carefully when submitting your work to Gradescope.\n",
    "\n",
    "## Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the homework, we ask that you **write your solutions individually**. If you do discuss the assignments with others, please **include their names** below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators**: _list collaborators here_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Assignment\n",
    "\n",
    "Welcome to Homework 3! For this assignment, we will work with Bloomberg news articles on Apple and Apple stock data (AAPL).\n",
    "\n",
    "In this assignment, you will gain practice with:\n",
    "\n",
    "- Conducting data cleaning and EDA on a text-based dataset,\n",
    "- Manipulating data in `pandas` with the `datetime` and `string` accessors,\n",
    "- Analyzing seaborn visualizations to better understand our data,\n",
    "- Writing regular expressions and using `pandas` RegEx methods, and\n",
    "- Performing sentiment analysis on text using DistilBERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook. \n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "\n",
    "import re\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ds100_utils import *\n",
    "\n",
    "# Ensure that pandas shows at least 280 characters in columns, so we can see full articles.\n",
    "pd.set_option(\"max_colwidth\", 280)\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will use the DistilBERT model, which is a Natural Language Processing (NLP) model designed to understand human language by processing text to capture the context and meaning of words within sentences. You are not expected to know the details of the model, but we will use it in this homework to perform sentiment analysis on textual data. We are importing those tools and the corresponding model below. **If you see any warnings, please ignore them. As long as the cell runs, there shouldn't be any issues.**\n",
    "\n",
    "## IMPORTANT NOTE\n",
    "\n",
    "Due to a finite amount of computing resources on DataHub, the cell below **may take a long time to run** (easily a couple minutes). This may also apply to other cells later in the assignment that load and use the NLP model. \n",
    "\n",
    "**Please be patient**, wait, and **avoid restarting the kernel or rerunning these cells** more than necessary. Not only will doing so make *your* notebook run slower, it can also affect *other students* on your CPU cluster. \n",
    "\n",
    "In addition, do NOT open this assignment in multiple tabs or windows. It can cause your notebook to crash, as well as affect Datahub. \n",
    "\n",
    "Please be patient and come to Office Hours or ask on Ed if you have any issues!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "model_checkpoint = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Breakdown\n",
    "\n",
    "Question | Manual| Points\n",
    "--- |---| ---\n",
    "1a |No| 1\n",
    "1b |No| 1\n",
    "1c |No| 1\n",
    "2ai |No| 2\n",
    "2aii |No| 1\n",
    "2b |No| 2\n",
    "2c |No| 1\n",
    "2di |No| 1\n",
    "2dii |Yes| 1\n",
    "3a |No| 1\n",
    "3bi |No| 1\n",
    "3bii |No| 1\n",
    "3c |Yes| 1\n",
    "3d |Yes| 1\n",
    "3ei |No| 1\n",
    "3eii | Yes | 1\n",
    "**Total** | **4** | **18**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Start\n",
    "\n",
    "For each question in the assignment, please write down your answer in the answer cell(s) right below the question.\n",
    "\n",
    "We understand that it is helpful to have extra cells breaking down the process towards reaching your final answer. If you happen to create new cells below your answer to run code, **NEVER** add cells between a question cell and the answer cell below it. It will cause errors when we run the autograder, and it will sometimes cause a failure to generate the PDF file.\n",
    "\n",
    "**Important note: The local autograder tests will not be comprehensive. You can pass the automated tests in your notebook but still fail tests on Gradescope after the grades are released.** Please be sure to check your results carefully.\n",
    "\n",
    "Finally, unless we state otherwise, **do not use for loops or list comprehensions**. The majority of this assignment can be done using built-in commands in `pandas` and `NumPy`.\n",
    "\n",
    "### Debugging Guide\n",
    "\n",
    "If you run into any technical issues, we highly recommend checking out the [Data 100 Debugging Guide](https://ds100.org/debugging-guide/). In this guide, you can find general questions about Datahub, Gradescope, and common `pandas` and RegEx errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 1: Importing the Data\n",
    "\n",
    "The data for this assignment is a subset of the financial news dataset from [this github repo](https://github.com/philipperemy/financial-news-dataset). The original datasets are no longer available online due to copyright issues, but we were allowed access for educational purposes. The data in the file `data/bloomberg_news.txt` has been filtered to just Bloomberg articles published between 2010 to 2013 (inclusive) with text that contains mentions of various popular stocks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1a\n",
    "\n",
    "Let's examine the contents of the `data/bloomberg_news.txt` file. Using the [`open` function](https://docs.python.org/3/library/functions.html#open) and [`read` operation](https://docs.python.org/3/tutorial/inputoutput.html#methods-of-file-objects) on a `python` file object, read **the first 1000 characters** in `data/bloomberg_news.txt` and store your result in the variable `q1a`. Then, display the result so you can read it.\n",
    "\n",
    "**CAUTION: Viewing the contents of large files in a Jupyter Notebook could crash your browser. Be careful not to print the entire contents of the file.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "print(q1a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1b\n",
    "\n",
    "Based on the printed output you got from `q1a`, what format is the data in? Answer this question by entering the letter corresponding to the right format in the variable `q1b` below.\n",
    "\n",
    "**CAUTION: As a reminder, viewing the contents of large files in a Jupyter Notebook could crash your browser. Be careful not to print the entire contents of the file, and do not use the file explorer to open data files directly.**\n",
    "\n",
    "**A.** CSV<br/>\n",
    "**B.** HTML<br/>\n",
    "**C.** JavaScript Object Notation (JSON)<br/>\n",
    "**D.** Excel XML\n",
    "\n",
    "Answer in the following cell. Your answer should be a string, either `\"A\"`, `\"B\"`, `\"C\"`, or `\"D\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q1b = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1c\n",
    "\n",
    "`pandas` has built-in readers for many different file formats, including the file format used here to store news articles. To learn more about these, check out the documentation for\n",
    "\n",
    "- `pd.read_csv` [(docs)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)\n",
    "- `pd.read_html`[(docs)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html)\n",
    "- `pd.read_json`[(docs)](https://pandas.pydata.org/docs/reference/api/pandas.read_json.html)\n",
    "- `pd.read_excel`[(docs)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html).\n",
    "\n",
    "For this question, use one of these functions to:\n",
    "1. Load the file `bloomberg_news.txt` in the data folder as a `DataFrame` into the variable `news_df`.\n",
    "2. Set the **index** of `news_df` to correspond to the `id` of each news article.\n",
    "\n",
    "\n",
    "**Hint:** If your code is taking a while to run, you should review your answers to `q1a` and `q1b`; you may have used the incorrect data loading function for the type of the given file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "news_df = ...\n",
    "news_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 2: Time Analysis\n",
    "\n",
    "After loading in the data, we can start exploring news articles by analyzing the relationships between the release dates (date of publication) and different topics and companies.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "### Question 2a, Part i\n",
    "\n",
    "First, let's extract the date and time from the `released_at` column in `news_df`. Notice that the date and time are encoded in the following format:\n",
    "\n",
    "```\n",
    "<date>May 29 2012</date> <time>09:40:58</time>\n",
    "<date>May 18 2011</date> <time>22:42:40</time>\n",
    "<date>August 15 2012</date> <time>00:09:02</time>\n",
    "<date>July 1 2011</date> <time>22:12:37</time>\n",
    "...\n",
    "```\n",
    "\n",
    "There are several ways to convert this to a `Timestamp` object that we can use more easily. However, for this assignment, we are going to use string manipulation functions. \n",
    "\n",
    "Create a regular expression that extracts the Month, Day, Year, Hour, Minute, and Second from the `news_df[\"released_at\"]` column. You should create a new `DataFrame` called `dates` that contains:\n",
    "1. The same index as `news_df` (`id`) and\n",
    "2. Column labels: `\"Month\"`, `\"Day\"`, `\"Year\"`, `\"Hour\"`, `\"Minute\"`, `\"Second\"`.\n",
    "\n",
    "Additionally, convert all numerical values (`\"Year\"`, `\"Day\"`, `\"Hour\"`, `\"Minute\"`, `\"Second\"`) to type `int`.\n",
    "\n",
    "**Hint 1:** You should use the `Series.str.extract` function ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.extract.html)).\n",
    "\n",
    "**Hint 2:** Don't forget to use raw strings and capture groups. Copy the above example text into [regex101.com](https://regex101.com/) to experiment with your regular expressions.\n",
    "\n",
    "**Hint 3:** It might be helpful to break this up into a couple of steps (e.g., first extract date values such as Month, Day, and Year and then extract time values such as Hour, Minute, and Second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 2a, Part ii\n",
    "\n",
    "Now that we extracted the dates and stored them into the `dates` Dataframe, let's merge `dates` with `news_df` so that we examine the trends in `news_df`over time. Assign `news_df_dates` to the merged dataframe.\n",
    "\n",
    "**Hint:** Use pd.merge [documentation]() to merge two dataframes. In order to pass the tests, `news_df` should be the *left* dataframe and `dates` should be the *right* dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "news_df_dates = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2aii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2b\n",
    "\n",
    "Now, let's analyze articles about different topics and companies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "For each company in the list of `companies` (provided below), add a boolean column to the `news_df` `DataFrame` indicating whether the corresponding company is mentioned in the `\"content\"` of the article. Ultimately, you should add six new columns containing `True`/`False` values to the `DataFrame`: `\"amazon\"`, `\"nintendo\"`, `\"apple\"`, `\"sony\"`, `\"facebook\"`, `\"netflix\"`. You may use a for loop over the list of companies.\n",
    "\n",
    "**Note:** Make the contents of the articles lowercase before searching for the keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "companies = [\"amazon\", \"nintendo\", \"apple\", \"sony\", \"facebook\", \"netflix\"]\n",
    "\n",
    "...\n",
    "\n",
    "news_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2c\n",
    "\n",
    "Now, we can put everything together to analyze the release dates and volume of articles for different companies.\n",
    "\n",
    "Create a new `DataFrame` called `company_mentions` that contains the number of articles mentioning each company in the list `companies` after 2010 (inclusive). `company_mentions` should have six columns (one column for each company), and the index of this `DataFrame` should be the release year `\"Year\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "company_mentions = ...\n",
    "company_mentions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Given your code in the previous part is correct, after running the cell below, you should be able to see the number of articles released mentioning `companies` for each year. The plot should look like this:\n",
    "<center>\n",
    "<img src = \"images/num_articles.png\" width = \"500\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "for company in companies:\n",
    "    sns.lineplot(data=company_mentions.reset_index(),\n",
    "                 x=\"Year\",\n",
    "                 y=company,\n",
    "                 label=company)\n",
    "plt.legend(fontsize=\"12\")\n",
    "plt.xticks(np.arange(2010, 2014), np.arange(2010, 2014))\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.title(\"Number of Articles Released (2010-2013)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "### Question 2d\n",
    "This graph is pretty interesting! Since Apple seems to have high rates of mentions in Bloomberg articles, let's examine Apple more closely. \n",
    "\n",
    "Let's start by loading the stock data for Apple into a `Dataframe` called `aapl_stock`. The stock data is stored in `data/aapl_stock_daily.csv`. We can use visualizations to further explore this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Run the cell below to load in the data, and see what the `Dataframe` contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "aapl_stock = pd.read_csv(\"data/aapl_stock_daily.csv\")\n",
    "aapl_stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 2d, Part i\n",
    "Change the datatype of Date column to be datetime, and set the index to the Date column. Then filter `aapl_stock` to only contain stock data from 2010 to 2013 (inclusive) so we can compare it to our data in part 2c.\n",
    "\n",
    "**Hint:** consider using `pd.to_datetime` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "aapl_stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing\n",
    "\n",
    "Now that `aapl_stock` is in the correct format, you should be able to visualize trends within the AAPL stock over time. Using `seaborn` we have provided code below that creates a line plot using `aapl_stock`, with the column `Close` as the y-axis and `Date` as the x-axis. Note how we always make sure to label our plot and axis!\n",
    "\n",
    "Run the cell below to display the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell, no further action needed here.\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.lineplot(data=aapl_stock.reset_index(), x='Date', y='Close')\n",
    "plt.title('Apple Stock Price')\n",
    "plt.ylabel('Close Price ($)')\n",
    "plt.xlabel('Year');\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Question 2d, Part ii\n",
    "\n",
    "Compare the plot above to the plot in Question 2c to better understand the history of Apple stock.\n",
    "\n",
    "<img src = \"images/num_articles.png\" width = \"350\"> <img src = \"images/aapl_stock.jpg\" width = \"350\">\n",
    "\n",
    "Does any specific year in the graphs stand out to you? Use a search engine search to explain at least one relevant event that could correspond to the time period you're curious about.\n",
    "\n",
    "**Hint:** Consider using the filtering capabilities of your search engine to narrow down your results (ex: only view articles that were published from 2011 to 2013)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 3: Sentiment Analysis\n",
    "\n",
    "In this section, we will continue building on our past analysis and specifically look at the **sentiment of each article** —— this will lead us to a much more direct and detailed understanding of how these articles can be used in different applications. **Sentiment analysis** is generally the computational task of classifying the emotions in a body of text as positively or negatively charged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We will use a fine-tuned version of the **DistilBERT** model ([github](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), [original paper](https://arxiv.org/abs/1910.01108)) to analyze the sentiment of Bloomberg news articles. DistilBERT is a neural network-based language model (a close relative to ChatGPT); we will use the model checkpoint specifically trained for sentiment analysis. These models are not in scope for Data 100, and we don't expect you to know how they work; take CS182: Neural Networks or Data 102: Data, Inference, and Decisions if you're interested in learning more. We are using them here to show how easy (and useful) these technologies have become.\n",
    "\n",
    "We can use the [HuggingFace library](https://huggingface.co/) to build the sentiment analysis pipeline and load the model. [Here](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english) is the card of the model checkpoint we will use for this assignment: the model card contains general information about the model, including the base model used, training arguments, training data, etc. Again, you don't need to know this for the course but knowing about model cards is important when you start to use these techniques in your careers.\n",
    "\n",
    "Run the following two cells to set up the sentiment analysis pipeline and see examples of how we can get the sentiment for different strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=model_checkpoint)\n",
    "\n",
    "# Get the sentiment of a given string\n",
    "sentiment_1 = sentiment_analysis(\"I have two dogs.\")\n",
    "print(\"Example 1: \" + str(sentiment_1))\n",
    "\n",
    "sentiment_2 = sentiment_analysis(\"I do not have dogs.\")\n",
    "print(\"Example 2: \" + str(sentiment_2))\n",
    "\n",
    "sentiment_3 = sentiment_analysis(\"Fortunately, I do not have dogs to worry about.\")\n",
    "print(\"Example 3: \" + str(sentiment_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "As you can see, the model can determine the sentiment of phrases/sentences (not just words). The model measures the phrase's **polarity**, indicating how strongly negative or positive it is on a scale of 0 to 1.\n",
    "\n",
    "**Note:** The output is a list, and each element of the list is a dictionary with two keys (label and score). Note that we could have gotten the sentiments of the two sentences by putting them in a list (batch) and then running the pipeline once (see the code below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "sentiments = sentiment_analysis([\"I have two dogs.\", \"I do not have dogs.\"])\n",
    "print(sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "### Question 3a\n",
    "\n",
    "Try it out yourself! The sentences we provided in the previous example have pretty high polarity scores. Let's see how the model behaves with more ambiguous sentences.\n",
    "\n",
    "Write a sentence `less_polar_sentence` that has a polarity score less than 0.8. This may take some trial and error. Let this be an opportunity to think about whether the model works as you'd expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "less_polar_sentence = ...\n",
    "results = sentiment_analysis(less_polar_sentence)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3b\n",
    "\n",
    "As running all the articles through the model will take a while, let's filter our articles to only look at articles about Apple. We currently have Bloomberg articles about all stocks and the dates of the respective articles in `news_df_dates`, so we'll need to filter this in part i.\n",
    "\n",
    "\n",
    "Due to model input size constraints, a maximum of 512 words (tokens), and limited computational resources on Datahub, we cannot load the full articles into the pipeline. Instead, we can look at the article headlines that mention Apple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 3b, Part i\n",
    "\n",
    "Assign `apple_re` to a regular expression that matches headlines referencing \"apple\" or \"aapl\" (in lowercase, as we will be following the common practice to lowercase text before searching for matches).\n",
    "\n",
    "You should develop and test your regular expression using [regex101.com](https://regex101.com/). Here are some practice sentences.\n",
    "\n",
    "```\n",
    "Sales of Macs and iPads Lead Apple’s Quarterly Revenue Up 5%\n",
    "```\n",
    "```\n",
    "Apple Jumps Into A.I. Fray With Apple Intelligence\n",
    "```\n",
    "\n",
    "\n",
    "Then:\n",
    "1. Canonicalize the `\"title\"` of the articles by converting the text to lowercase,\n",
    "2. Use the `apple_re` regular expression to only include rows that have titles that match `apple_re`\n",
    "3. Assign the filtered rows to `aaple_news_title`\n",
    "\n",
    "\n",
    "**Hint 1:** `Series.str.contains` function might be useful \n",
    "\n",
    "**Hint 2:** Consider using non-capturing groups, which allows you to group entire subexpressions `r\"(:?\\subexpression1 | \\subexpression2)\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "apple_re = ...\n",
    "...\n",
    "\n",
    "aapl_news_title.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3bi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 3b, Part ii\n",
    "\n",
    "Using the `sentiment_analysis` model, we can now determine the sentiment of the titles that mention \"apple\" or \"aapl\" for each article. \n",
    "\n",
    "However, having all 1000+ students in Data 100 run the model for all titles has in the past been too computationally heavy for DataHub to easily handle. So we have done this part for you and saved the immediate results of the following code to a csv file, `appl_sentiment.csv`:\n",
    "\n",
    "`pd.DataFrame(sentiment_analysis(aapl_news_title[\"title\"].tolist()), index = aapl_news_title.index)`\n",
    "\n",
    "**You should not run the above code yourself. This may overload DataHub and cause crashes.** \n",
    "\n",
    "1. Load the file `aapl_sentiment.csv` in the data folder as a `DataFrame` into the variable `sentiment`. Feel free to reference question 1c for a reminder on ways to read in data.\n",
    "2. Set the index of `sentiment` to correspond to the `id` of each article.\n",
    "\n",
    "Note that the model outputs both a label and a score. After importing the data, we recommend taking a look at its structure before attempting the next part.\n",
    "\n",
    "3. From the outputted label and score, compute a sentiment score for each title. If the label is \"POSITIVE\" the sentiment score should just be the score listed. If the label is \"NEGATIVE\" the sentiment score should be the score converted to a negative number. \n",
    "\n",
    "4. Add a new column `'title_sentiment'` to `aapl_news_title` with the computed sentiment score values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "aapl_news_title.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3bii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 3c\n",
    "\n",
    "Now that we've used our model to evaluate the sentiment of the titles in `aapl_news_title` let's use visualizations to better understand the sentiment distribution. \n",
    "\n",
    "#### Visualizing\n",
    "\n",
    "Using `seaborn`, we created a histogram to visualize the distribution of `title_sentiment`. Run the cell below to display the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = aapl_news_title, x = 'title_sentiment')\n",
    "plt.xlabel('Sentiment of Titles')\n",
    "plt.title('Histogram of Title Sentiment')\n",
    "plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Analyzing\n",
    "\n",
    "Given your understanding of the model from part **3a** (where you created sentences with a lower polarity score) are you surprised by the distribution of sentiment in the graph above? Describe what you notice about the graph and how it relates to what you learned in part **3a**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "### Continued Visualizing\n",
    "Let's continue to explore `aapl_news_title` using data visualization. If you run the cell below, we add a new column `'date'` to `aapl_news_title` which concatenates the Year, Month, and Day columns into one column in `datetime` format. This format will help when it comes to visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the month names to numbers\n",
    "month_map = {\n",
    "    'January': 1, 'February': 2, 'March': 3, 'April': 4,\n",
    "    'May': 5, 'June': 6, 'July': 7, 'August': 8,\n",
    "    'September': 9, 'October': 10, 'November': 11, 'December': 12\n",
    "}\n",
    "# Apply the mapping to the Month column\n",
    "aapl_news_title['Month'] = aapl_news_title['Month'].map(month_map)\n",
    "# Ensure Year, Month, and Day are all strings\n",
    "aapl_news_title['Year'] = aapl_news_title['Year'].astype(str)\n",
    "aapl_news_title['Month'] = aapl_news_title['Month'].astype(str)\n",
    "aapl_news_title['Day'] = aapl_news_title['Day'].astype(str)\n",
    "\n",
    "# Combine the columns into a single date string in 'YYYY-MM-DD' format\n",
    "aapl_news_title['date_str'] = aapl_news_title['Year'] + '-' + aapl_news_title['Month'] + '-' + aapl_news_title['Day']\n",
    "\n",
    "# Convert the combined string to a datetime object using pd.to_datetime()\n",
    "aapl_news_title['date'] = pd.to_datetime(aapl_news_title['date_str'], format='%Y-%m-%d', errors='coerce')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a line plot to visualize the change in sentiment in `aapl_news_title` over time, using `sns.lineplot` to plot `date` as the x-axis and `title_sentiment` as the y-axis. Run the cell below to display the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data = aapl_news_title, x = 'date', y = 'title_sentiment')\n",
    "plt.xticks(rotation=70);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This plot is not very pretty!** This isn't because of any errors on your part, instead, the graph tells us that we might need to use a different visualization method to understand our data, which we'll do below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 3d\n",
    "\n",
    "Let's find a better way to visualize our data. We can still use `sns.lineplot`, but instead of graphing every single row, let's find a way to aggregate our data.\n",
    "1. Using `pd.groupby` [(documentation)](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) find the average sentiment of every year-month pair in `aapl_news_title`.\n",
    "2. Store the result in `monthly_avg_sentiment`.\n",
    "3. Add a column `'date'` that holds the datetime objects for each Year-Month.\n",
    "4. Run the cell to plot the averages.\n",
    "\n",
    "**Hint**: For step 3, you may find `pd.to_datetime` helpful. Because this function requires a Day in addition to Month and Year to create datetime objects, you will need to manually `.assign(Day=1)` to set the day to the first day of the month.\n",
    "\n",
    "Your graph should look similar to the one below:\n",
    "\n",
    "<img src = \"images/sentiment_graph.png\" width = \"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "monthly_avg_sentiment = ...\n",
    "monthly_avg_sentiment['date'] = ...\n",
    "\n",
    "# If the above are implemented correctly, running this cell should produce the graph shown above.\n",
    "\n",
    "sns.lineplot(data = monthly_avg_sentiment, x = 'date', y = 'title_sentiment');\n",
    "plt.title('Avg. Sentiment Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Title Sentiment')\n",
    "plt.axhline(0, color='red')\n",
    "plt.xticks(rotation = 80);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "This graph is a lot easier for us to read! Looking at the graph, it seems that the number of headlines mentioning Apple or AAPL with positive sentiments decreased from 2010 to early 2012, during which point sentiment remained consistently low. We see that the sentiment begins to increase again in late 2013. This could perhaps relate to the drop and subsequent uptick in Apple's stock trends that we saw in Question 2. Here is the graph below:\n",
    "\n",
    "<img src = \"images/aapl_stock.jpg\" width = \"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "### Question 3e\n",
    "\n",
    "Let's audit some of our data to better understand how the sentiment analysis model works with our dataset. This is common practice when working with models! It's good practice to compare our own assumptions to the conclusions our model is drawing to ensure we're using the correct model for our tasks.\n",
    "\n",
    "#### Question 3e, Part i\n",
    "\n",
    "Let's look into the titles of the articles a bit more closely. Assign `top_positive` and `top_negative` to dataframes containing the five sentences with the highest `title_sentiment` and the five sentences with the lowest `title_sentiment` respectively. The dataframes should have the columns `title` and `title_sentiment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_positive = ...\n",
    "top_negative = ...\n",
    "\n",
    "display(top_positive, top_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3ei\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Question 3e, Part ii\n",
    "\n",
    "Do you agree with the current sentiment-based ordering of headlines, or would you rearrange the ordering? Do you feel that the DistilBERT model is a good model for our task of analyzing sentiment in news headlines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Takeaways\n",
    "\n",
    "In this homework, we used an LLM to evaluate the sentiment of news articles quantify text data (qualitative data) so that we could perform data analysis on a large set of journalism data. We were able to see how news articles reflected trends seen in the stock market through the sentiment of the headlines. Though we used the [HuggingFace DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english) model, there are thousands of NLP models available for use, and with rapid innovations in the NLP research space, there are new models frequently being created. In fact, we were using a different model for this homework one year ago, which reflects how quickly the NLP field progresses. \n",
    "\n",
    "Different models evaluate sentiment differently. You may have noticed that the DistilBERT model struggles with evaluating neutral sentences and often gives sentences a high polarity score. When evaluating which models to use in your projects, it's useful to test them on small inputs of data to see how they perform, like we did by testing out various sentences! Different models may perform differently (often due to how the model was trained and created) so it's important to understand these differences when deciding what model to use for your data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Missy says congratulations! You have finished Homework 3!\n",
    "\n",
    "<img src = \"images/missy.jpg\" width = \"200\">\n",
    "\n",
    "### Course Content Feedback\n",
    "\n",
    "If you have any feedback about this assignment or about any of our other weekly, weekly assignments, lectures, or discussions, please fill out the [Course Content Feedback Form](https://docs.google.com/forms/d/e/1FAIpQLSe0fBEJwt6aEfZxU3fh3llNk8rSWHj6Umq0km3wPqmFu0MlGA/viewform). Your input is valuable in helping us improve the quality and relevance of our content to better meet your needs and expectations!\n",
    "\n",
    "### Submission Instructions\n",
    "\n",
    "Below, you will see a cell. Running this cell will automatically generate a zip file with your autograded answers. Once you submit this file to the HW 3 Coding assignment on Gradescope, Gradescope will automatically submit a PDF file with your written answers to the HW 3 Written assignment. If you run into any issues when running this cell, feel free to check this [section](https://ds100.org/debugging-guide/autograder_gradescope/autograder_gradescope.html#why-does-grader.exportrun_teststrue-fail-if-all-previous-tests-passed) in the Data 100 Debugging Guide.\n",
    "\n",
    "**Important**: Please check that your written responses were generated and submitted correctly to the HW 3 Written Assignment.\n",
    "\n",
    "**You are responsible for ensuring your submission follows our requirements and that the PDF for HW 3 written answers was generated/submitted correctly. We will not be granting regrade requests nor extensions to submissions that don't follow instructions.** If you encounter any difficulties with submission, please don't hesitate to reach out to staff prior to the deadline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "otter": {
   "OK_FORMAT": true,
   "require_no_pdf_confirmation": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q1a) == 1000\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q1a.startswith('[{\"id\":46243185,\"title\":\"Ope')\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 'and  Apple Inc. (AAPL)' in q1a\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> q1b.upper() in ['A', 'B', 'C', 'D']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> news_df.index.name == 'id'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(['title', 'released_at', 'content', 'path']) == set(news_df.columns)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> news_df.shape == (4635, 4)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.random.seed(100)\n>>> expected = set([10511979, 35357277, 49432195, 52444917, 70540777, 80595106, 86731456, 88502258, 91893641, 97354949])\n>>> set(np.random.choice(sorted(news_df.index), replace=False, size=10)) == expected\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2ai": {
     "name": "q2ai",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> {'Day', 'Hour', 'Minute', 'Month', 'Second', 'Year'} == set(dates.columns)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(dates['Month'].unique()) == {'April', 'August', 'November', 'December', 'February', 'May', 'October', 'September', 'January', 'June', 'March', 'July'}\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> dates['Year'].dtype == int\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> dates['Day'].dtype == int\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> dates['Hour'].dtype == int\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> dates['Minute'].dtype == int\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> dates['Second'].dtype == int\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2aii": {
     "name": "q2aii",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> set(news_df_dates.columns) == {'title', 'released_at', 'content', 'path', 'Month', 'Day', 'Year', 'Hour', 'Minute', 'Second'}\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> news_df.shape[0] == 4635\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(news_df.columns) == {'amazon', 'apple', 'content', 'facebook', 'netflix', 'nintendo', 'path', 'released_at', 'sony', 'title'}\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c": {
     "name": "q2c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> company_mentions.shape == (4, 6)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(company_mentions.columns) == {'amazon', 'apple', 'facebook', 'netflix', 'nintendo', 'sony'}\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(company_mentions.index) == {2010, 2011, 2012, 2013}\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2di": {
     "name": "q2di",
     "points": 1,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> (sentiment_analysis(less_polar_sentence)[0]['score'] < 0.8) == True\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3bi": {
     "name": "q3bi",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> aapl_news_title.shape == (474, 10)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> re.findall(apple_re, 'hello') == []\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3bii": {
     "name": "q3bii",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 'title_sentiment' in aapl_news_title.columns\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> aapl_news_title.shape == (474, 11)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(aapl_news_title.columns) == {'title', 'released_at', 'content', 'path', 'Month', 'Day', 'Year', 'Hour', 'Minute', 'Second', 'title_sentiment'}\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> aapl_news_title['title_sentiment'].iloc[1] <= 1 and aapl_news_title['title_sentiment'].iloc[1] >= -1\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3ei": {
     "name": "q3ei",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> top_positive.shape[0] == 5\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> top_negative.shape[0] == 5\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
